{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# open text file and read\n",
    "with open(r\"C:\\Users\\Gokul\\Downloads\\anna.txt\", 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# encoding and mapping text to int and vice versa\n",
    "chars = tuple(set(text))                           #83 chars\n",
    "int2char = dict(enumerate(chars))                  #mapping char to unique integers\n",
    "char2int = {ch: i for i, ch in int2char.items()}   #mapping integers to char\n",
    "\n",
    "# encode text\n",
    "encoded = np.array([char2int[ch] for ch in text])  #encoding all char in txt to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 4, 5]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[3,4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Preprocessing - creating one-hot vectors\n",
    "def one_hot_encode(arr, vector_len):\n",
    "\n",
    "    #initialize array\n",
    "    one_hot = np.zeros((arr.size, vector_len), dtype = np.float32)\n",
    "    #print(one_hot.shape[0])\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1\n",
    "    #print(arr.shape, *arr.shape)\n",
    "    #reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, vector_len))\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# To check whether the one_hot_encode function works as expected\n",
    "test_seq = np.array([[3, 5, 1]]) # 2 dimensional\n",
    "one_hot_trial = one_hot_encode(test_seq, 8)\n",
    "print(one_hot_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    # arr = [1,2,..13], batch_size = 2 -> [1,2,..6],[7,8,..,12]\n",
    "    # batch_size - 2 and seq_length - num of elements to go in a batch = 3\n",
    "    total_batch_size = batch_size * seq_length  # 2*3 = 6\n",
    "    n_batch = len(arr) // total_batch_size      # num of batches = 13//6 = 2\n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:(n_batch * total_batch_size)]    # now arr = [1,2,..12]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape(batch_size, -1)\n",
    "    #print(arr.shape)\n",
    "\n",
    "    # Iterate over the batches using a window of size seq_length\n",
    "    for i in range(0, arr.shape[1], seq_length):\n",
    "        # features\n",
    "        x = arr[:, i:i+seq_length]\n",
    "        # targets - shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        # whole thing below is a wrap-around - last element of y is first element of x\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, i+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        # yield is generator func, gives o/p generator object, can be read using for loop\n",
    "        yield x, y\n",
    "    #return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[40 44 58 77 76 32 59 14 65 26]\n",
      " [81 55 45 14 76 44 58 76 14 58]\n",
      " [32 45 16 14 55 59 14 58 14 11]\n",
      " [81 14 76 44 32 14 82 44 34 32]\n",
      " [14 81 58 66 14 44 32 59 14 76]\n",
      " [82 46 81 81 34 55 45 14 58 45]\n",
      " [14 36 45 45 58 14 44 58 16 14]\n",
      " [49 52  1 55 45 81 51 80  5 14]]\n",
      "\n",
      "y\n",
      " [[44 58 77 76 32 59 14 65 26 26]\n",
      " [55 45 14 76 44 58 76 14 58 76]\n",
      " [45 16 14 55 59 14 58 14 11 55]\n",
      " [14 76 44 32 14 82 44 34 32 11]\n",
      " [81 58 66 14 44 32 59 14 76 32]\n",
      " [46 81 81 34 55 45 14 58 45 16]\n",
      " [36 45 45 58 14 44 58 16 14 81]\n",
      " [52  1 55 45 81 51 80  5 14 67]]\n"
     ]
    }
   ],
   "source": [
    "# To check whether the get_batches function works as expected\n",
    "batches = get_batches(encoded, batch_size = 8, seq_length = 50)\n",
    "x, y = next(batches)\n",
    "# printing out the first 10 items in a sequence\n",
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# RNN\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2, drop_prob = 0.5, lr = 0.001):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        self.drop_prob = drop_prob\n",
    "        self.lr = lr\n",
    "\n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: i for i, ch in self.int2char.items()}\n",
    "\n",
    "        # LSTM(input, hidden, layers, dropout, batch_first)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, dropout = drop_prob, batch_first = True)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        # Linear(hidden, output)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        r_out, hidden = self.lstm(x, hidden)\n",
    "        out = self.dropout(r_out)\n",
    "        # change shape for FC layer\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(n_layers, batch_size, self.n_hidden).zero_(), weight.new(n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, data, epochs=5, batch_size=10, seq_length=50, lr=0.001, clip=4, val_frac=0.1, print_every=10):\n",
    "    model.train()\n",
    "    # loss and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "\n",
    "    # Creating training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "\n",
    "    counter = 0\n",
    "    n_chars = len(model.chars)\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        h = model.init_hidden(batch_size)\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "\n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            output, h = model(inputs,h)\n",
    "            loss = loss_func(output, targets.view(batch_size*seq_length).long())\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            # prevents from exploding gradient problem\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = model.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "\n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    inputs, targets = x, y\n",
    "\n",
    "                    output, val_h = model(inputs, val_h)\n",
    "                    val_loss = loss_func(output, targets.view(batch_size*seq_length).long())\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "                model.train() # reset to train mode after iterating through validation data\n",
    "\n",
    "                print(\"Epoch: {}/{}...\".format(epoch+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.001)\n",
      "  (dropout): Dropout(p=0.001, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "n_hidden = 512\n",
    "n_layers = 2\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 3\n",
    "lr = 0.001\n",
    "print_every = 10\n",
    "\n",
    "model = RNN(chars, n_hidden, n_layers, lr)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [35:07<00:00, 702.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 10... Loss: 3.2230... Val Loss: 3.2260\n",
      "Epoch: 1/3... Step: 20... Loss: 3.1178... Val Loss: 3.1430\n",
      "Epoch: 1/3... Step: 30... Loss: 3.1130... Val Loss: 3.1271\n",
      "Epoch: 1/3... Step: 40... Loss: 3.0887... Val Loss: 3.1213\n",
      "Epoch: 1/3... Step: 50... Loss: 3.1223... Val Loss: 3.1200\n",
      "Epoch: 1/3... Step: 60... Loss: 3.1007... Val Loss: 3.1176\n",
      "Epoch: 1/3... Step: 70... Loss: 3.0913... Val Loss: 3.1175\n",
      "Epoch: 1/3... Step: 80... Loss: 3.1096... Val Loss: 3.1130\n",
      "Epoch: 1/3... Step: 90... Loss: 3.1046... Val Loss: 3.1037\n",
      "Epoch: 1/3... Step: 100... Loss: 3.0771... Val Loss: 3.0807\n",
      "Epoch: 1/3... Step: 110... Loss: 3.0323... Val Loss: 3.0254\n",
      "Epoch: 1/3... Step: 120... Loss: 2.9315... Val Loss: 2.9375\n",
      "Epoch: 1/3... Step: 130... Loss: 2.9602... Val Loss: 2.8542\n",
      "Epoch: 2/3... Step: 140... Loss: 2.7949... Val Loss: 2.7726\n",
      "Epoch: 2/3... Step: 150... Loss: 2.6988... Val Loss: 2.9193\n",
      "Epoch: 2/3... Step: 160... Loss: 2.6476... Val Loss: 2.6562\n",
      "Epoch: 2/3... Step: 170... Loss: 2.5511... Val Loss: 2.5812\n",
      "Epoch: 2/3... Step: 180... Loss: 2.5014... Val Loss: 2.5296\n",
      "Epoch: 2/3... Step: 190... Loss: 2.4456... Val Loss: 2.4862\n",
      "Epoch: 2/3... Step: 200... Loss: 2.4513... Val Loss: 2.4693\n",
      "Epoch: 2/3... Step: 210... Loss: 2.3997... Val Loss: 2.4327\n",
      "Epoch: 2/3... Step: 220... Loss: 2.3723... Val Loss: 2.4034\n",
      "Epoch: 2/3... Step: 230... Loss: 2.3518... Val Loss: 2.3764\n",
      "Epoch: 2/3... Step: 240... Loss: 2.3390... Val Loss: 2.3578\n",
      "Epoch: 2/3... Step: 250... Loss: 2.2960... Val Loss: 2.3406\n",
      "Epoch: 2/3... Step: 260... Loss: 2.2641... Val Loss: 2.3133\n",
      "Epoch: 2/3... Step: 270... Loss: 2.2528... Val Loss: 2.2870\n",
      "Epoch: 3/3... Step: 280... Loss: 2.2456... Val Loss: 2.2619\n",
      "Epoch: 3/3... Step: 290... Loss: 2.2066... Val Loss: 2.2377\n",
      "Epoch: 3/3... Step: 300... Loss: 2.1751... Val Loss: 2.2146\n",
      "Epoch: 3/3... Step: 310... Loss: 2.1468... Val Loss: 2.1966\n",
      "Epoch: 3/3... Step: 320... Loss: 2.1110... Val Loss: 2.1719\n",
      "Epoch: 3/3... Step: 330... Loss: 2.0815... Val Loss: 2.1598\n",
      "Epoch: 3/3... Step: 340... Loss: 2.0890... Val Loss: 2.1466\n",
      "Epoch: 3/3... Step: 350... Loss: 2.0809... Val Loss: 2.1171\n",
      "Epoch: 3/3... Step: 360... Loss: 1.9927... Val Loss: 2.0934\n",
      "Epoch: 3/3... Step: 370... Loss: 2.0247... Val Loss: 2.0774\n",
      "Epoch: 3/3... Step: 380... Loss: 1.9967... Val Loss: 2.0514\n",
      "Epoch: 3/3... Step: 390... Loss: 1.9649... Val Loss: 2.0374\n",
      "Epoch: 3/3... Step: 400... Loss: 1.9317... Val Loss: 2.0211\n",
      "Epoch: 3/3... Step: 410... Loss: 1.9409... Val Loss: 2.0032\n"
     ]
    }
   ],
   "source": [
    "train(model, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=lr, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Top-k sampling used\n",
    "def predict(model, char, h = None, top_k = None):\n",
    "    #tensor inputs\n",
    "    x = np.array([[model.char2int[char]]])\n",
    "    x = one_hot_encode(x, len(model.chars))\n",
    "    inputs = torch.from_numpy(x)\n",
    "\n",
    "    h = tuple([each.data for each in h])\n",
    "    out, h = model(inputs, h)\n",
    "    # getting char probabilities\n",
    "    p = func.softmax(out, dim=1).data\n",
    "    #get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(len(model.chars))\n",
    "    else:\n",
    "        p, top_ch = p.topk(top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "\n",
    "    # select the likely next character with some element of randomness\n",
    "    p = p.numpy().squeeze()\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "    # return the encoded value of the predicted char and hidden state\n",
    "    return model.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def sample(model, size, prime='The', top_k = None):\n",
    "    model.eval()  #evaluation mode\n",
    "\n",
    "    # Running through prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = model.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(model, ch, h, top_k = top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "\n",
    "    # Passing prev char and getting new one\n",
    "    for i in range(size):\n",
    "        char, h = predict(model, chars[-1], h, top_k = top_k)\n",
    "        chars.append(char)\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annad of and has soming. \"To shiss therester, and shith the said his fentiting of him. He sad\n",
      "tha dersed the could of a preated this tha dead as the cardor her his dacking harders ander, hit\n",
      "attoor her the mastere to beat the parer thing har hould the sad thome thine han stale and andor the\n",
      "heas to be that and her, and and has deas frongh with of ham all the sis astere as the hang his, wat\n",
      "her hore thoughing\n",
      "sould and har at to ham\n",
      "anes, he\n",
      "prostions and to\n",
      "the sered to sade to\n",
      "beno he\n",
      "werlits of then stere has hen at han a froos at had. He\n",
      "sing alle and anding and thet and steled oun he sin the was, and the\n",
      "pound har her astire his soont har her. The pas than somate, and all the serting to be what his was ate han astelled.\"\n",
      "\n",
      "\"The wis ofer ater, and alk the sad as and ther the her, all, be atten his, andad his santed to thind tho sander. He countren,\n",
      "shis her atrowe him thin sout in ther. The proster aster sime, and, and she hid not in the simithing tith her here to thoughter a to to to ham\n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Due to computational limitation, only few epochs were run but as we can see from the above paragraph of text generated, which minimal training, some of the commonly used words are generated without any spelling mistakes. With more training epochs, we can generate text with complete sense and no errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
