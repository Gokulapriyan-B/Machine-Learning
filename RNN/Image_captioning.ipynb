{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd                             # csv\n",
    "import numpy as np\n",
    "import spacy                                    # nlp\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence     # sequence padding\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image                           # image\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models             # inception v3\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Building vocabulary from the dataset\n",
    "spacy_eng = spacy.load('en_core_web_sm')\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.idx2str = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UKN>'} # UKN - unknown, if a word has freq less than threshold then it is mapped to UKN\n",
    "        self.str2idx = {'<PAD>':0, '<SOS>':1, '<EOS>':2, '<UKN>':3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2str)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [i.text.lower() for i in spacy_eng.tokenizer(text)]\n",
    "        #above converts 'My Name is' to ['my', 'name', 'is']\n",
    "\n",
    "    def build_vocabulary(self, sentence_list):\n",
    "        freq = {}\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word not in freq:\n",
    "                    freq[word] = 1\n",
    "                else:\n",
    "                    freq[word] += 1\n",
    "\n",
    "                if freq[word] == self.freq_threshold:\n",
    "                    self.str2idx[word] = idx\n",
    "                    self.idx2str[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "\n",
    "        return [\n",
    "            self.str2idx[token] if token in self.str2idx else self.str2idx['<UKN>']\n",
    "            for token in tokenized_text\n",
    "        ]\n",
    "        # above return respective index for word if above threshold else return 3 - <UKN>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Accessing the dataset and pre-processing images and captions as per requirement\n",
    "class model_data(Dataset):\n",
    "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=1):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = pd.read_csv(captions_file)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.imgs = self.df['image']\n",
    "        self.captions = self.df['caption']\n",
    "\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        img_id = self.imgs[index]\n",
    "        img = Image.open(os.path.join(self.root_dir, img_id)).convert('RGB')\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        caption2int = [self.vocab.str2idx[\"<SOS>\"]]\n",
    "        caption2int += self.vocab.numericalize(caption)\n",
    "        caption2int.append(self.vocab.str2idx['<EOS>'])\n",
    "\n",
    "        return img, torch.tensor(caption2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Padding the sequence for equal length of inputs using this function instead of default collate function as concat of images and text is absent in default function\n",
    "class pad_seq:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        targets = [item[1] for item in batch]\n",
    "        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return imgs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# splitting and creating datasets\n",
    "def get_loader(root_folder, csv_file, transform): #batch_size=32, num_workers=8, shuffle=True):\n",
    "    dataset = model_data(root_folder, csv_file, transform=transform)\n",
    "    pad_idx = dataset.vocab.str2idx['<PAD>']\n",
    "    total_count = len(dataset)\n",
    "    print(total_count)\n",
    "    train_count = int(0.8 * total_count)\n",
    "    print(train_count)\n",
    "    valid_count = int(0.1 * total_count)\n",
    "    print(valid_count)\n",
    "    test_count = total_count - train_count - valid_count\n",
    "    print(test_count)\n",
    "    train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(dataset, (train_count, valid_count, test_count))\n",
    "    # loader = DataLoader(dataset=dataset,\n",
    "    #                     batch_size=batch_size,\n",
    "    #                     num_workers=num_workers,\n",
    "    #                     shuffle=shuffle,\n",
    "    #                     collate_fn=pad_seq(pad_idx=pad_idx))\n",
    "    return dataset, train_dataset, valid_dataset, test_dataset, pad_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Encoder_CNN(nn.Module):\n",
    "    def __init__(self, embed_dim, train_CNN=False):\n",
    "        super(Encoder_CNN, self).__init__()\n",
    "        self.train_CNN = train_CNN\n",
    "        self.inception = models.inception_v3(pretrained=True, aux_logits=False)\n",
    "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.times = []\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, images):\n",
    "        with torch.no_grad():\n",
    "            features = self.inception(images)\n",
    "        return self.dropout(self.relu(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Decoder_RNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, n_layers):\n",
    "        super(Decoder_RNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, n_layers)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.dropout(self.embed(captions))\n",
    "        # concatenate output features of image with caption embeddings\n",
    "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
    "        hidden, _ = self.lstm(embeddings)\n",
    "        output = self.linear(hidden)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CNN2RNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, n_layers):\n",
    "        super(CNN2RNN, self).__init__()\n",
    "        self.encoderCNN = Encoder_CNN(embed_dim)\n",
    "        self.decoderRNN = Decoder_RNN(embed_dim, hidden_dim, vocab_size, n_layers)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoderCNN(images)\n",
    "        output = self.decoderRNN(features, captions)\n",
    "        return output\n",
    "\n",
    "    def caption_image(self, image, vocabulary, max_length=50):\n",
    "        result_caption = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.encoderCNN(image).unsqueeze(0) # unsqueeze for batch size\n",
    "            states = None # states for lstm\n",
    "\n",
    "            for _ in range(max_length):\n",
    "                hidden, states = self.decoderRNN.lstm(x, states)\n",
    "                output = self.decoderRNN.linear(hidden.squeeze(0))\n",
    "                predicted = output.argmax(1)\n",
    "\n",
    "                result_caption.append(predicted.item())\n",
    "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
    "\n",
    "                if vocabulary.idx2str[predicted.item()] == '<EOS>':\n",
    "                    break\n",
    "        return [vocabulary.idx2str[idx] for idx in result_caption]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss_track = []\n",
    "def train(model, train_loader, valid_loader, dataset, epochs=5, lr=0.001, print_every=25):\n",
    "\n",
    "    train_CNN = False\n",
    "    counter = 0\n",
    "    \n",
    "    loss_func = nn.CrossEntropyLoss(ignore_index=dataset.vocab.str2idx['<PAD>'])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # To train whole model or just fine tune to our need\n",
    "    for name, param in model.encoderCNN.inception.named_parameters():\n",
    "            if 'fc.weight' in name or 'fc.bias' in name:\n",
    "                param.requires_grad = True\n",
    "            else:\n",
    "                param.requires_grad = train_CNN\n",
    "\n",
    "    model.train()\n",
    "    for epoch in (range(epochs)):\n",
    "        for idx, (imgs, captions) in enumerate(train_loader):#, total=len(train_loader), leave=False):\n",
    "            counter += 1\n",
    "\n",
    "            output = model(imgs, captions[:-1])\n",
    "            # output -> [seq_len, N, vocab_size], for loss -> [seq_len*N, vocab_size]\n",
    "            loss = loss_func(output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_track.append(loss.item())\n",
    "            \n",
    "            # loss stats\n",
    "            if counter%print_every == 0:\n",
    "                val_losses = []\n",
    "                model.eval()\n",
    "                for idx, (imgs, captions) in enumerate(valid_loader):#, total=len(valid_loader), leave=False):\n",
    "                    output = model(imgs, captions[:-1])\n",
    "                    val_loss = loss_func(output.reshape(-1, output.shape[2]), captions.reshape(-1))\n",
    "                    val_losses.append(val_loss.item())\n",
    "                model.train()\n",
    "                print('Epoch: {}/{}...'.format(epoch+1, epochs),\n",
    "                      'Step: {}...'.format(counter),\n",
    "                      'Loss: {:.6f}...'.format(loss.item()),\n",
    "                      'Valid_loss: {:.6f}...'.format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "40\n",
      "5\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((356, 356)),\n",
    "            transforms.RandomCrop((299, 299)), # this specific dim used cuz same used in paper\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "\n",
    "dataset, train_dataset, valid_dataset, test_dataset, pad_idx = get_loader(r\"E:\\img_captioning_data\\flickr8k\\images_test\",\n",
    "                                                                          r\"E:/img_captioning_data/flickr8k/captions_test.txt\",\n",
    "                                                                          transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "embed_dim = 256\n",
    "hidden_dim = 256\n",
    "vocab_size = len(dataset.vocab)\n",
    "n_layers = 2\n",
    "lr = 0.001\n",
    "print_every = 20\n",
    "epochs = 50\n",
    "batch_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "train_loader = DataLoader(dataset=train_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=pad_seq(pad_idx=pad_idx))\n",
    "\n",
    "valid_loader = DataLoader(dataset=valid_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=pad_seq(pad_idx=pad_idx))\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=pad_seq(pad_idx=pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN2RNN(\n",
      "  (encoderCNN): Encoder_CNN(\n",
      "    (inception): Inception3(\n",
      "      (Conv2d_1a_3x3): BasicConv2d(\n",
      "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (Conv2d_2a_3x3): BasicConv2d(\n",
      "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (Conv2d_2b_3x3): BasicConv2d(\n",
      "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (Conv2d_3b_1x1): BasicConv2d(\n",
      "        (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (Conv2d_4a_3x3): BasicConv2d(\n",
      "        (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (Mixed_5b): InceptionA(\n",
      "        (branch1x1): BasicConv2d(\n",
      "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch5x5_1): BasicConv2d(\n",
      "          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch5x5_2): BasicConv2d(\n",
      "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_1): BasicConv2d(\n",
      "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_2): BasicConv2d(\n",
      "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_3): BasicConv2d(\n",
      "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch_pool): BasicConv2d(\n",
      "          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (Mixed_5c): InceptionA(\n",
      "        (branch1x1): BasicConv2d(\n",
      "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch5x5_1): BasicConv2d(\n",
      "          (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch5x5_2): BasicConv2d(\n",
      "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_1): BasicConv2d(\n",
      "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_2): BasicConv2d(\n",
      "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_3): BasicConv2d(\n",
      "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch_pool): BasicConv2d(\n",
      "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (Mixed_5d): InceptionA(\n",
      "        (branch1x1): BasicConv2d(\n",
      "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch5x5_1): BasicConv2d(\n",
      "          (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch5x5_2): BasicConv2d(\n",
      "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_1): BasicConv2d(\n",
      "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_2): BasicConv2d(\n",
      "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_3): BasicConv2d(\n",
      "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch_pool): BasicConv2d(\n",
      "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (Mixed_6a): InceptionB(\n",
      "        (branch3x3): BasicConv2d(\n",
      "          (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_1): BasicConv2d(\n",
      "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_2): BasicConv2d(\n",
      "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_3): BasicConv2d(\n",
      "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (Mixed_6b): InceptionC(\n",
      "        (branch1x1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_2): BasicConv2d(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_3): BasicConv2d(\n",
      "          (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_2): BasicConv2d(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_3): BasicConv2d(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_4): BasicConv2d(\n",
      "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_5): BasicConv2d(\n",
      "          (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch_pool): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (Mixed_6c): InceptionC(\n",
      "        (branch1x1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_2): BasicConv2d(\n",
      "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_3): BasicConv2d(\n",
      "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_2): BasicConv2d(\n",
      "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_3): BasicConv2d(\n",
      "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_4): BasicConv2d(\n",
      "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_5): BasicConv2d(\n",
      "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch_pool): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (Mixed_6d): InceptionC(\n",
      "        (branch1x1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_2): BasicConv2d(\n",
      "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_3): BasicConv2d(\n",
      "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_2): BasicConv2d(\n",
      "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_3): BasicConv2d(\n",
      "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_4): BasicConv2d(\n",
      "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_5): BasicConv2d(\n",
      "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch_pool): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (Mixed_6e): InceptionC(\n",
      "        (branch1x1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_2): BasicConv2d(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7_3): BasicConv2d(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_2): BasicConv2d(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_3): BasicConv2d(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_4): BasicConv2d(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7dbl_5): BasicConv2d(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch_pool): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (Mixed_7a): InceptionD(\n",
      "        (branch3x3_1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3_2): BasicConv2d(\n",
      "          (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7x3_1): BasicConv2d(\n",
      "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7x3_2): BasicConv2d(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7x3_3): BasicConv2d(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch7x7x3_4): BasicConv2d(\n",
      "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (Mixed_7b): InceptionE(\n",
      "        (branch1x1): BasicConv2d(\n",
      "          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3_1): BasicConv2d(\n",
      "          (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3_2a): BasicConv2d(\n",
      "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3_2b): BasicConv2d(\n",
      "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_1): BasicConv2d(\n",
      "          (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_2): BasicConv2d(\n",
      "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_3a): BasicConv2d(\n",
      "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_3b): BasicConv2d(\n",
      "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch_pool): BasicConv2d(\n",
      "          (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (Mixed_7c): InceptionE(\n",
      "        (branch1x1): BasicConv2d(\n",
      "          (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3_1): BasicConv2d(\n",
      "          (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3_2a): BasicConv2d(\n",
      "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3_2b): BasicConv2d(\n",
      "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_1): BasicConv2d(\n",
      "          (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_2): BasicConv2d(\n",
      "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_3a): BasicConv2d(\n",
      "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch3x3dbl_3b): BasicConv2d(\n",
      "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "        (branch_pool): BasicConv2d(\n",
      "          (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        )\n",
      "      )\n",
      "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "      (dropout): Dropout(p=0.5, inplace=False)\n",
      "      (fc): Linear(in_features=2048, out_features=256, bias=True)\n",
      "    )\n",
      "    (relu): ReLU()\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoderRNN): Decoder_RNN(\n",
      "    (embed): Embedding(191, 256)\n",
      "    (lstm): LSTM(256, 256, num_layers=2)\n",
      "    (linear): Linear(in_features=256, out_features=191, bias=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = CNN2RNN(embed_dim, hidden_dim, vocab_size, n_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/50... Step: 20... Loss: 4.567641... Valid_loss: 4.775859...\n",
      "Epoch: 2/50... Step: 40... Loss: 4.389285... Valid_loss: 4.585564...\n",
      "Epoch: 3/50... Step: 60... Loss: 4.089845... Valid_loss: 4.488102...\n",
      "Epoch: 4/50... Step: 80... Loss: 3.918598... Valid_loss: 4.451128...\n",
      "Epoch: 5/50... Step: 100... Loss: 2.985101... Valid_loss: 4.357591...\n",
      "Epoch: 6/50... Step: 120... Loss: 2.478770... Valid_loss: 4.038241...\n",
      "Epoch: 7/50... Step: 140... Loss: 2.521892... Valid_loss: 3.978059...\n",
      "Epoch: 8/50... Step: 160... Loss: 2.104608... Valid_loss: 4.159768...\n",
      "Epoch: 9/50... Step: 180... Loss: 1.637854... Valid_loss: 3.997078...\n",
      "Epoch: 10/50... Step: 200... Loss: 1.636634... Valid_loss: 4.287665...\n",
      "Epoch: 11/50... Step: 220... Loss: 1.626146... Valid_loss: 4.108763...\n",
      "Epoch: 12/50... Step: 240... Loss: 1.358522... Valid_loss: 4.126743...\n",
      "Epoch: 13/50... Step: 260... Loss: 0.866801... Valid_loss: 4.284659...\n",
      "Epoch: 14/50... Step: 280... Loss: 1.249933... Valid_loss: 4.102302...\n",
      "Epoch: 15/50... Step: 300... Loss: 1.384343... Valid_loss: 4.328449...\n",
      "Epoch: 16/50... Step: 320... Loss: 0.911152... Valid_loss: 4.151868...\n",
      "Epoch: 17/50... Step: 340... Loss: 0.798948... Valid_loss: 4.417699...\n",
      "Epoch: 18/50... Step: 360... Loss: 0.784992... Valid_loss: 4.289994...\n",
      "Epoch: 19/50... Step: 380... Loss: 0.762971... Valid_loss: 4.476958...\n",
      "Epoch: 20/50... Step: 400... Loss: 0.645693... Valid_loss: 4.467147...\n",
      "Epoch: 21/50... Step: 420... Loss: 0.615521... Valid_loss: 4.550222...\n",
      "Epoch: 22/50... Step: 440... Loss: 0.511837... Valid_loss: 4.376254...\n",
      "Epoch: 23/50... Step: 460... Loss: 0.563501... Valid_loss: 4.614422...\n",
      "Epoch: 24/50... Step: 480... Loss: 0.572559... Valid_loss: 4.729699...\n",
      "Epoch: 25/50... Step: 500... Loss: 0.450489... Valid_loss: 4.756558...\n",
      "Epoch: 26/50... Step: 520... Loss: 0.478160... Valid_loss: 4.532872...\n",
      "Epoch: 27/50... Step: 540... Loss: 0.468857... Valid_loss: 4.810113...\n",
      "Epoch: 28/50... Step: 560... Loss: 0.447055... Valid_loss: 4.812411...\n",
      "Epoch: 29/50... Step: 580... Loss: 0.438089... Valid_loss: 4.869699...\n",
      "Epoch: 30/50... Step: 600... Loss: 0.444837... Valid_loss: 4.775232...\n",
      "Epoch: 31/50... Step: 620... Loss: 0.389046... Valid_loss: 4.796202...\n",
      "Epoch: 32/50... Step: 640... Loss: 0.341426... Valid_loss: 4.898588...\n",
      "Epoch: 33/50... Step: 660... Loss: 0.355856... Valid_loss: 4.900504...\n",
      "Epoch: 34/50... Step: 680... Loss: 0.400333... Valid_loss: 4.960575...\n",
      "Epoch: 35/50... Step: 700... Loss: 0.353011... Valid_loss: 4.981466...\n",
      "Epoch: 36/50... Step: 720... Loss: 0.345299... Valid_loss: 4.925398...\n",
      "Epoch: 37/50... Step: 740... Loss: 0.300279... Valid_loss: 4.809630...\n",
      "Epoch: 38/50... Step: 760... Loss: 0.389749... Valid_loss: 5.033080...\n",
      "Epoch: 39/50... Step: 780... Loss: 0.358098... Valid_loss: 5.058946...\n",
      "Epoch: 40/50... Step: 800... Loss: 0.382555... Valid_loss: 4.925579...\n",
      "Epoch: 41/50... Step: 820... Loss: 0.325281... Valid_loss: 5.105583...\n",
      "Epoch: 42/50... Step: 840... Loss: 0.342313... Valid_loss: 4.869205...\n",
      "Epoch: 43/50... Step: 860... Loss: 0.305559... Valid_loss: 4.923144...\n",
      "Epoch: 44/50... Step: 880... Loss: 0.365845... Valid_loss: 5.029542...\n",
      "Epoch: 45/50... Step: 900... Loss: 0.375484... Valid_loss: 5.145730...\n",
      "Epoch: 46/50... Step: 920... Loss: 0.375878... Valid_loss: 5.134778...\n",
      "Epoch: 47/50... Step: 940... Loss: 0.290607... Valid_loss: 5.201438...\n",
      "Epoch: 48/50... Step: 960... Loss: 0.354821... Valid_loss: 5.271804...\n",
      "Epoch: 49/50... Step: 980... Loss: 0.353875... Valid_loss: 5.202832...\n",
      "Epoch: 50/50... Step: 1000... Loss: 0.319604... Valid_loss: 5.254592...\n"
     ]
    }
   ],
   "source": [
    "train(model, train_loader, valid_loader, dataset, epochs=epochs, lr=lr, print_every=print_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the training losses\n",
    "plt.plot(range(len(loss_track)), loss_track)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training loss history')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "####  Due to computational limitaion, I have trained this model on a dataset of 10 images for 50 epochs. We can observe that as we are overfitting the model over 10 images, we achieve a loss that is less than 0.5.  We can infer from the above graph that the model is working fine and with the model trained over the whole dataset for sufficient amount of epochs, good accuracy will be achieved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
